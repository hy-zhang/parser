\section{Overview}\label{sec:overview}

This section gives an overview of our library \name, from a user's perspective. Basically, \name consists of four parts: underlying parsing technique, delegation mechanism encoded by open recursion, Object Algebras, and glue code of new combinators and utility functions. We start from section \ref{subsec:overview-parsing}, which discusses the choice of parsing technique and how it affects modularity of parsers. Section \ref{subsec:overview-problem} demonstrates the goal of extending parsers together with ASTs in a semantic modular way, with both separate compilation and type-safe code reuse. Then we will see traditional parser combinators fail to achieve it because of hard-coded recursive calls. In section \ref{subsec:overview-delegation}, we show how delegation can solve this problem and allow us to build extensible parsers. Finally, section \ref{subsec:overview-oa} gives examples of using Object Algebras for more extensibility, including extension of operations and parsing multiple sorts of syntax.

\huang{done} \bruno{Write a summary for this section}

\huang{done}\bruno{Fill in all the missing references}

\subsection{Choosing the Parsing Technique}\label{subsec:overview-parsing}

%A technique for type-safe modular parsing should the following 3
%features: \emph{modular type-checking}; \emph{separate compilation};
%low performance overhead

Although there are many parsing techniques, not all of them are
suitable for type-safe modular parsing. In particular there are many
techniques which fail to provide modular type-checking and separate
compilation. Moreover, even if modular type-checking and separate
compilation are supported, efficiency is another
concern. A parsing technique should have low overhead when applied
in a modular setting. In the remaider of this section, we eliminate
various techniques that fail to satisfy our requirements, and argue
that that Packrat parsing~\cite{Ford2002} is a suitable candidate for
type-safe modular parsing.

\paragraph{Parser Generators} The most widely use tools for parsing
are parser generators. Parser generators help users generate parsers automatically or
semi-automatically from a given grammar. There is no restriction on
the algorithm, while most of them adopts table-based LL~\cite{lewis1968syntax} and LR~\cite{knuth1965translation} parsing
algorithms.
Although efficient, the main drawback of parser generators is that they do not support
modular type-checking and separate compilation.

Modular parsing based on parser generators is supported by many libraries~\cite{antlr1995,Grimm2006,Gouseti2014,Warth2016}. Users can separate the syntax definition and related parsing code into reusable components. Then the corresponding parsers are built by their library utilities. For example, NOA~\cite{Gouseti2014} uses Java annotation processing to collect grammar information, and then generates ANTLR4 parsers. However, such generation procedure requires a whole compilation after the collection of all grammar pieces. Once the grammar changed, even slightly, grammar information must be re-collected and the parser must be re-generated. Hence those libraries only have syntactic modularity.

Generating parsers often requires full information of grammars, thus semantic modularity is difficult to achieve in this way.

\paragraph{Parser Combinators}
Comparing with the parser generators, a \textit{parser combinator}~\cite{burge1975,Wadler1985}
takes several parsers and produce a new parser as its output. Parser combinators are
popular in functional programming, where the parsers are represented
by functions and parser combinators are higher-order functions accepts
them.

At a first look, parser combinators are very suitable for our purpose, because of two
reasons. Firstly, they are naturally modular. The manner of using them
is to write small parsers and use combinators to composed them
together. The construction procedure is explicit and fully controlled
by the programmer. Secondly, each parser combinator is represented by
a piece of code, and also are the parsers it takes. Thus in a
statically typed programming language they can be statically
type-checked.

Unfortunately many parser combinators have important limitations for
modular parsing. In particular several parser combinators,
including the famous library Parsec~\cite{Leijen2001} library, require
programmers to manually do \textit{left-recursion elimination}, and
require significant amounts \textit{backtracking}. Both of these are
problematic in a modular setting.

\paragraph{Left-Recursion Elimination} The top-down, recursive descent parsing strategy adopted by those parser combinator libraries cannot support left-recursive grammars directly. The common solution is to rewrite the grammar into an equivalent but not left-recursive one, so called left-recursion elimination.

A left-recursive grammar of adding integers can be rewritten as below. On the right side, we got a new grammar without left-recursion.

\begin{tabular}{m{0.4\linewidth}m{0.4\linewidth}}
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> \alt <expr> `+' <int>
\end{grammar}
&
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> <expr'>

<expr'> ::= <empty> \alt `+' <int> <expr'>
\end{grammar}
\end{tabular}

The main problem with left-recursion elimination is that it is a
\emph{global} transformation on a grammar. Given that the full grammar
is known, then it is possible to remove all left-recursive
cases. However, when doing modular parsing, the full grammar is not
known!

For the example above, if we extend the original grammar to support
subtraction, we must analyse the full grammar again to rewrite it.

\begin{tabular}{m{0.4\linewidth}m{0.4\linewidth}}
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> \alt <expr> `+' <int> \alt <expr> `-' <int>
\end{grammar}
&
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> <expr'>

<expr'> ::= <empty> \alt `+' <int> <expr'> \alt `-' <int> <expr'>
\end{grammar}
\end{tabular}


Another issue of left-recursion elimination is that it requires extra
bookkeeping work to retain the original semantics. For example, the
expression $1+2-3$ is parsed as $(1+2)-3$ in the left-recursive
grammar, but after rewrite the result is $1((+2)-3))$. The parse tree
must be transformed to recover its structure.

\huang{todo}
\bruno{This example does not appear to be problematic: adding ``-''
  does not affect the rest of the grammar. Isn't the problem that
  sometimes adding a new case to the grammar, requires changing other
parts? Otherwise I don't see why left-recursion is a problem, and we
cannot simply claim that it is a problem. You have to make a better
effort to find a problematic grammar.
}

\paragraph{Backtracking} The need for backtracking is also problematic
in a modular setting. For example, suppose that we only have \inlinecode{import..from} statements before, now we want to extend the grammar to add an \inlinecode{import..as} case. The third line of grammar below is the new case.

\setlength{\grammarindent}{5em}
\begin{grammar}
<stmt> ::= `import' <ident> `from' <ident>
    \alt ...
    \alt `import' <ident> `as' <ident>
\end{grammar}

Because the \inlinecode{import..from} case shares a prefix with the new case \inlinecode{import..as}, when the former case fails, we must backtrack to the beginning. Take Parsec as an example, its choice
combinator \inlinecode{<|>} only tries the second alternative if the first fails
without any token consumption. An auxiliary function \inlinecode{try} is used for explicit backtracking.

\begin{lstlisting}[language=PlainCode]
oldParser = parseImpFrom <|> parseA <|> parseB <|> ...
newParser = try parseImpFrom <|> parseA <|> parseB <|> ... <|> parseImpAs
\end{lstlisting}

Given the full grammar, we can decide where to put \inlinecode{try} for backtracking. However, in a modular setting we should always consider the worst case, that all alternatives may share common prefixes with future cases. Then we need to backtrack for all the branches, which results worst case exponential time complexity.
In Parsec, we must add \inlinecode{try} everywhere:

\begin{lstlisting}[language=PlainCode]
parser = try parseImpFrom <|> try parseA <|> try parseB <|> ... <|> try parseImpAs
\end{lstlisting}

\huang{I've rewritten this paragraph}\bruno{The example is good, I think but the explanation is not.
You want to say that when you know
  the full grammar, you can figure out where ``try'' is
  needed. Without the full grammar you'd need assume the worst
  case. Make an effort to make your explanation cristal clear!
I think you want to miss the abstract explanation that you give first,
with the explanation about the concrete example.
Start with ``The need for backtracking is also problematic
in a modular setting. For example, suppose ...'' and synchronize
the abstract explanation and the explanation for the example.
}

\paragraph{Packrat Parsing}
Fortunately more advanced parsing techniques such as Packrat
parsing~\cite{Ford2002}, address the limitations of simple parser combinators
such as Parsec. Packrat parsers use
memoization to record the result of applying each parser at each
position of the input, so that repeated computation is eliminated, and
they support left-recursive grammars directly~\cite{warth2008}. All of these properties are very
suitable for modularity, thus we use Packrat parsers as the underlying
parsing technique. In section \ref{sec:packratparsers}, we will give
an overview of the Packrat parsing library implemented in Scala,
which we use.

It is worth mentioning that the choice of parser combinators will not
affect the other parts of our library. One can choose other parser
combinators like Parsec, in cases that the performance and supporting
of left-recursion are not major concerns.

\subsection{The Parsing Expression Problem}\label{subsec:overview-problem}
\huang{done, added the attempt/failure/reason of extending conventional parsers}\bruno{You are not motivating the problem! You are going straight to
  the solution without pointing out what the problem is first. What
  you need to do is: First show what happens with conventional
  parsers: at some point, if you add extensions the recursive calls
  will be wrong. Then you show (in the next section) the solution:
 use delegation/open recursion.}

\huang{partly done, only extend a new language construct in the example, how about leaving extension of new operations in the OA subsection?}\bruno{I think we need to set up a challenge here, similar to the challenge of
  the expression problem. The challenge should be like. Build a parser
for a simple expression language, then extend the language with both
a new language construct and a new operation. This section will show
how we can do that using traditional parsers, but without modular
type-safety and separate compilation. The remaining sections will show
that the techniques introduced by us, enable us to solve those two challenges.}

Typical parsers defined parser combinators follow the grammar
structure, and build a corresponding AST. If the grammar is changed to
add a new alternative, the corresponding parser has to be changed as
well. Thus the traditional style of developing parsers is not
extensible, and it is at odds with object-oriented programming ASTs,
which support the easy addition of new alternative classes. We
illustrate this problem with a simple expression language and extension.

Suppose we want to parse a simple language of variables and
applications.

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <ident>
    \alt <expr> <expr>
    \alt `(' <expr> `)'
\end{grammar}

It is straightforward to model ASTs by inheritance and write corresponding parsers for each case. \huang{This is the first time to demonstrate code in this paper. Readers may not know what those combinators/types are. Refer to the library docs here? move the 'scala parsing library' section to the beginning as 'background'?}

\lstinputlisting[linerange=8-18]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_SIMPLE_EXPR

As the combination of \inlinecode{pVar} and \inlinecode{pApp}, \inlinecode{pExpr} can parse expressions correctly.

\paragraph{Attempt to Extend the Parser} Now consider we are extending the abstract syntax as well as the parsers. We add lambda abstractions as a new case of expressions. The language then becomes the famous \textit{untyped lambda calculus}.

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= ...
    \alt `\\' <ident> `.' <expr>
\end{grammar}

The ASTs can be extended trivially as below.

\lstinputlisting[linerange=22-22]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_SIMPLE_LAM

Since we already have the parser for variables and applications, we would like to build the new parser by reusing the old one. So we compose \inlinecode{pLam}, which parses lambda abstractions, with the old parser. \inlinecode{pExtExpr} is the new function to parse expressions.

\lstinputlisting[linerange=26-32]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_SIMPLE_EXT

Unfortunately, this extended parser does not work properly to parse all possible expressions defined by the new grammar. For example, it fails to parse \inlinecode{(\\x.x)} \inlinecode{(\\y.y)} which is obviously valid.

\paragraph{Problem: Hard-coded Recursive Calls} The critical problem of extending parsers directly is that the recursive calls in old parsers are hard-coded. Specifically, \inlinecode{pApp} makes two recursive calls to parse two consecutive expressions as an application. Before the extension, we implemented it by using \inlinecode{pExpr} for those recursive calls. It worked fine at that time because \inlinecode{pExpr} covers all cases of the old grammar. However, after the extension of lambda abstractions, the proper function for such recursive calls should be \inlinecode{pExtExpr} instead of \inlinecode{pExpr}, because the new function \inlinecode{pExtExpr} covers all of the three cases in the new grammar, including the lambda case.

That is the reason why the extended parser fails to parse some valid expressions such as \inlinecode{(\\x.x)} \inlinecode{(\\y.y)}. To resolve this issue, we need to let \inlinecode{pApp} be aware of the change of grammar. We have some direct solutions:

\begin{itemize}
    \item Keep the old code, rewrite problematic functions like \inlinecode{pApp}. \\Separate compilation is obtained but code reuse is lost, because every function may contain such recursive calls.
    \item Modify the old code, correct the recursive calls by proper functions. \\We can avoid code duplication thus get code reuse, but separate compilation is lost because of modification on the old code.
    \item ??? \huang{third one? use dynamic languages?}
\end{itemize}

Similar to the trade-off in the Expression Problem, neither of them satisfies us. The challenge of building semantically modular parsers requires fundamental change of the parser structure. We will introduce our solution in the next sections.

\subsection{Delegation-Based Parsing}\label{subsec:overview-delegation}

Hard-coded recursive calls prevent our parsers from extensibility.
This problem can be solved using delegation, which can be encoded with
\textit{open recursion}. The idea is simple: instead of making direct
recursive calls, an additional argument, that abstracts over the
recursive call, is passed to the parsers. This technique enables the
developement of parsers that support OO-style extensibility of ASTs.

\paragraph{Open Parsing} Consider the expression language of only variables and applications in the previous section. The parser can be rewritten as follows:

\lstinputlisting[linerange=53-63]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_BASE

According to the definition of type synonym \inlinecode{Open[T]},
every parse function takes a parser of \inlinecode{Expr} as a
parameter, namely \inlinecode{self}. This parameter \inlinecode{self}
represents \emph{the parser} of expressions, which is dynamiclly
explained by the argument. Using an abstract parameter provides
extensibility to parsers, because the recursive calls are not
hard-coded here --- they are dynamic and can be changed corresponding
to the grammar!

We take \inlinecode{pApp} as an example again. It uses
\inlinecode{self} instead of hard-coded \inlinecode{pExpr} for
recursive parsing. Depending on the real argument, \inlinecode{self}
can be a parser which covers only variables and applications, or a
parser which also supports the lambda case.

\paragraph{Client Code} Since the type of parsing functions are
changed, we can not apply them directly. A \inlinecode{fix} function, which is the
the standard \textit{fixpoint combinator}, is used to `close' the open
recursion. After that, it can be used as an ordinary parser.
The client code below demonstrates the usage of new parsers.

\huang{done}\bruno{Too much detail is being given here about open
  recursion. In Section 2 we should talk only about \emph{how} to use
open recursion, and not how open recursion is implemented. We
can summarize, for example the API related to open recursion.
Section 4 is where the implementation and detailed explanation
about open recursion should come.
}

\lstinputlisting[linerange=68-72]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_USE

\huang{done}\bruno{Please polish up code. You probably want to consistently use
the variable name ``self'' whenever you want to refer to the whole
parser (instead of ``e'' that you use at the moment). Also you should
use ``Open'' (which you currently call ``Fix'') instead of using the
longwinded ``(=> PackratParser[Expr]) => PackratParser[Expr]''}

\huang{done}\bruno{Also, for consistency, why isn't ``pVar'' open? I.e. using a
  self reference as well? I know that it is not recursive, but giving
  it a different treatment makes it more confusing for users.}

\huang{done}\bruno{You should use a more OO approach in the example.
Rather than case classes show an Expr trait with pretty printing.
This will enable you to talk about the extensibility and object
algebras better later on.
}

\paragraph{Extensibility} As same as what we did in previous section, we extend the grammar by adding lambda abstractions as a new case. Similarly, the new parser extends from the old one by adding \inlinecode{pLam} on it. Notice that the old code is not touched, hence separate compilation is obtained.

\lstinputlisting[linerange=77-83]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_EXT

All recursive calls of parsing `an expression' are called via the explicit parameter \inlinecode{self}. Once the \inlinecode{fix} function is applied, these calls will be updated appropriately. Parsing functions such as \inlinecode{pApp} do not need to be rewritten. As a consequence, this parser recognizes all valid expressions of the new grammar, including \inlinecode{(\\x.x)} \inlinecode{(\\y.y)} which cannot be parsed in the previous section.

\lstinputlisting[linerange=89-89]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_EXT_USE

As we demonstrated, delegation encoded by open recursion is the key technique to obtain semantic modularity. It enables type-safe code reuse and separate compilation in our parsers. We will discuss this topic further in section \ref{sec:openandparsing}.

\subsection{Object Algebras for full Extensibility}\label{subsec:overview-oa}

Although delegation enables OO extensibility, the use of an OO class
hierarchy makes the addition of new operations over the AST
problematic. Object Algebras~\cite{Oliveira2012} enable us to solve this problem, and
offer high flexibility in the choice of operations to be performed
over the AST. It also makes parsing with multiple sorts of syntax easier.

\paragraph{Parsing with Object Algebras} Using Object Algebras, the abstract syntax of the language of variables and applications is defined as below.

\lstinputlisting[linerange=8-11]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_ALG

Then we are able to define operations over the syntax in an extensible way. Here we write a pretty printing operation for this language.

\lstinputlisting[linerange=15-18]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_PRINT

And also parser for it as below. Notice the parsing function \inlinecode{pExpr} takes an argument of type \inlinecode{ExprAlg[E]}, that means it accepts any instance of the algebra interface \inlinecode{ExprAlg}.

\lstinputlisting[linerange=22-28]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_PARSER

\paragraph{Extensibility of Syntax} Following previous examples, the code below shows how to extend the language to support lambda abstractions.

\lstinputlisting[linerange=32-46]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_EXT

For easily combining parsers using Object Algebras as ASTs, we have a special combinator \huang{we need to rename our combinator} defined in our library.

To use the parser, we must provide an algebra instance as the operation to construct the parsing results. In the code below, we use the pretty printing operation \inlinecode{LambdaPrint} as the algebra instance, so that the parsing result is a pretty printed AST.

\lstinputlisting[linerange=55-60]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_USE

\paragraph{Extensibility of Operations} With Object Algebras, operations over ASTs can also be extended in a modular way. Here is an example of collecting free variables from an expression. We can use this operation during parsing and obtain a set of free variables.

\lstinputlisting[linerange=74-80]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_EXT_OP


\paragraph{Multiple Sorts of Syntax} Another advantage of using Object Algebras is that it supports multiple sorts of syntax easily. In several cases, we want to divide the syntactic elements into some groups. For example, the grammar below has two sorts, which are expressions and types.

\begin{tabular}{m{0.4\linewidth}m{0.4\linewidth}}
\setlength{\grammarindent}{5em}
\begin{grammar}
<type> ::= `int' \alt <type> `->' <type>
\end{grammar}
&
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <ident> \alt <expr> <expr> \alt `\\' <ident> `.' <expr>
\end{grammar}
\end{tabular}

Using Object Algebras, we can easily distinguish them in an extensible way, just by adding an extra type parameter.

\lstinputlisting[linerange=65-69]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_MULTI_SYNTAX

