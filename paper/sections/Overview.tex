\section{Overview}\label{sec:overview}

\subsection{Choosing the Parsing Technique}\label{subsec:overview-parsing}

%A technique for type-safe modular parsing should the following 3
%features: \emph{modular type-checking}; \emph{separate compilation};
%low performance overhead

Although there are many parsing techniques, not all of them are
suitable for type-safe modular parsing. In particular there are many
techniques which fail to provide modular type-checking and separate
compilation. Moreover, even if modular type-checking and separate
compilation are supported, efficiency is another
concern. A parsing technique should have low overhead when applied
in a modular setting. In the remaider of this section, we eliminate
various techniques that fail to satisfy our requirements, and argue
that that Packrat parsing~\cite{} is a suitable candidate for
type-safe modular parsing.

\paragraph{Parser Generators} The most widely use tools for parsing
are parser generators~\cite{}. Parser generators help users generate parsers automatically or
semi-automatically from a given grammar. There is no restriction on
the algorithm, while most of them adopts table-based LL~\cite{} and LR parsing
algorithms~\cite{}.
Although efficient, the main drawback of parser generators is that they do not support
modular type-checking and separate compilation.

Modular parsing based on parser generators is supported by many libraries []. Users can separate the concrete/abstract syntax and related parsing code into reusable components. Then the corresponding parsers are built by their library utilities. For example, NOA [] uses Java annotation processing to collect grammar information, and then generates ANTLR4 parsers. However, such generation procedure requires a whole compilation after the collection of all grammar pieces. Once the grammar changed, even slightly, grammar information must be re-collected and the parser must be re-generated. Hence thoes libraries only have syntactic modularity.

Generating parsers often requires full information of grammars, thus semantic modularity is difficult to achieve in this way.

\huang{a paragraph here, refer to related work section, discuss
  previous works of modular parsing using parser generators and its
  drawbacks}\bruno{Please finish this.}\huang{done}

\paragraph{Parser Combinators}
Comparing with the parser generators, a \textit{parser combinator}~\cite{}
takes several parsers and produce a new parser as its output. Parser combinators are
popular in functional programming, where the parsers are represented
by functions and parser combinators are higher-order functions accepts
them.

At a first look, parser combinators are very suitable for our purpose, because of two
reasons. Firstly, they are naturally modular. The manner of using them
is to write small parsers and use combinators to composed them
together. The construction procedure is explicit and fully controlled
by the programmer. Secondly, each parser combinator is represented by
a piece of code, and also are the parsers it takes. Thus in a
statically typed programming language they can be statically
type-checked.

Unfortunatelly many parser combinators have important limitations for
modular parsing. In particular several parser combinators,
including the famous library Parsec~\cite{} library, require
programmers to manually do \textit{left-recursion elimination}, and
require significant amounts \textit{backtracking}. Both of these are
problematic in a modular setting.

\paragraph{Left-Recursion Elimination} The main problem with
left-recursion elimination is that it is a \emph{global}
transformation on a grammar. Given that the full grammar is known,
then it is possible to remove all left-recursive cases.
\bruno{Illustrate with a concrete example: show a simple
  left-recursive grammar; then do left-recursion elimination. Then add
a new case into the original left-recursive grammar; and show that
a very different grammar is obtained after left-recursion elimination.}

However, when doing modular parsing, the full grammar is not known!


Parser combinators adopt top-down, recursive descent parsing, which
uses backtracking to search through the possible branches. However,
simple backtracking parser combinators such as the famous library
Parsec~\cite{} in Haskell, have some drawbacks. One is that they cannot
support left-recursive grammars. The common solution is to rewrite a
left-recursive grammar into an equivalent one, so called
\textit{left-recursion elimination}. This solution requires
information of the whole grammar, so that it cannot be applied in a
modular setting. Because the grammar may be extended in the future,
essentially we only have parts of the grammar.

\paragraph{Backtracking} The need for backtracking is also problematic
in a modular setting. Take Parsec as an example, its choice
combinator only tries the second alternative if the first fails
without any token consumption. If two alternatives have a same
non-empty prefix, an auxiliary function \lstinline{try} must be added
to backtrack. However, if we want to extend our parsers later, we
should always consider the worst case in which all alternatives share
common prefixes. Then we need to add \lstinline{try} for all the
branches, which results worst case exponential time complexity.
\bruno{you should again illustrate your point here with an
  example. The example should illustrate that the addition of a new
  case may require introducing backtracking. }

\paragraph{Packrat Parsing}
Fortunatelly more advanced parsing techniques such as Packrat
parsing~\cite{}, address the limitations of simple parser combinators
such as Parsec. Packrat parsers use
memorization to record the result of applying each parser at each
position of the input, so that repeated computation is eliminated, and
they supports left-recursive grammars directly. All of these properties are very
suitable for modularity, thus we use Packrat parsers as the underlying
parsing technique. In section \ref{sec:packratparsers}, we will give
an overview of the Packrat parsing library implemented in Scala,
which we use.

It is worth mentioning that the choice of parser combinators will not
affect the other parts of our library. One can choose other parser
combinators like Parsec, in cases that the performance and supporting
of left-recursion are not major concerns.

\subsection{The Parsing Expression Problem}
\huang{done, added the attempt/failure/reason of extending conventional parsers}\bruno{You are not motivating the problem! You are going straight to
  the solution without pointing out what the problem is first. What
  you need to do is: First show what happens with conventional
  parsers: at some point, if you add extensions the recursive calls
  will be wrong. Then you show (in the next section) the solution:
 use delegation/open recursion.}

\huang{partly done, only extend a new language construct in the example, how about leaving extension of new operations in the OA subsection?}\bruno{I think we need to set up a challenge here, similar to the challange of
  the expression problem. The challange should be like. Build a parser
for a simple expression language, then extend the language with both
a new language construct and a new operation. This section will show
how we can do that using traditional parsers, but without modular
type-safety and separate compilation. The remaining sections will show
that the techniques introduced by us, enable us to solve those two challenges.}

Typical parsers defined parser combinators follow the grammar
structure, and build a corresponding AST. If the grammar is changed to
add a new alternative, the corresponding parser has to be changed as
well. Thus the traditional style of developing parsers is not
extensible, and it is at odds with object-oriented programming ASTs,
which support the easy addition of new alternative classes. We
illustrate this problem with a simple expression language and extension.

Suppose we want to parse a simple language of variables and
applications.

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <var>
    \alt <expr> <expr>
    \alt `(' <expr> `)'
\end{grammar}

It is straightforward to model ASTs by inheritance and write corresponding parsers for each case. \huang{This is the first time to demonstrate code in this paper. Readers may not know what those combinators/types are. Refer to the library docs here? move the 'scala parsing library' section to the beginning as 'background'?}

\begin{lstlisting}
abstract class Expr
class Var(x: String) extends Expr
class App(e1: Expr, e2: Expr) extends Expr

trait BaseParser {
  lexical.delimiters += ("(", ")")
  val pExpr: PackratParser[Expr] = pVar ||| pApp ||| "(" ~> pExpr <~ ")"
  val pVar: PackratParser[Expr] = ident ^^ { x => new Var(x) }
  val pApp: PackratParser[Expr] =
    pExpr ~ pExpr ^^ { case e1 ~ e2 => new App(e1, e2) }
}
\end{lstlisting}

As the combination of \lstinline{pVar} and \lstinline{pApp}, \lstinline{pExpr} can parse expressions correctly.

\paragraph{Attempt to Extend the Parser} Now consider we are extending the abstract syntax as well as the parsers. We add lambda abstractions as a new case of expressions. The language then becomes the famous \textit{untyped lambda calculus}.

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= ...
    \alt `\\' <var> `.' <expr>
\end{grammar}

The ASTs can be extended trivially as below.

\begin{lstlisting}
class Lam(x: String, e: Expr) extends Expr
\end{lstlisting}

Since we already have the parser for variables and applications, we would like to build the new parser by reusing the old one. So we compose \lstinline{pLam}, which parses lambda abstractions, with the old parser. \lstinline{pExtExpr} is the new function to parse expressions.

\begin{lstlisting}
trait ExtParser extends BaseParser {
  lexical.delimiters += ("\\", ".")
  val pExtExpr: PackratParser[Expr] =
    pVar ||| pApp ||| pLam ||| "(" ~> pExtExpr <~ ")"
  val pLam: PackratParser[Expr] =
    "\\" ~> ident ~ ("." ~> pExtExpr) ^^ { case x ~ e => new Lam(x, e) }
}
\end{lstlisting}

Unfortunately, this extended parser does not work properly to parse all possible expressions defined by the new grammar. For example, it fails to parse \lstinline{(\x.x)} \lstinline{(\y.y)} which is obviously valid.

\paragraph{Problem: Hard-coded Recursive Calls} The critical problem of extending parsers directly is that the recursive calls in old parsers are hard-coded. Specifically, \lstinline{pApp} makes two recursive calls to parse two consecutive expressions as an application. Before the extension, we implemented it by using \lstinline{pExpr} for those recursive calls. It worked fine at that time because \lstinline{pExpr} covers all cases of the old grammar. However, after the extension of lambda abstractions, the proper function for such recursive calls should be \lstinline{pExtExpr} instead of \lstinline{pExpr}, because the new function \lstinline{pExtExpr} covers all of the three cases in the new grammar, including the lambda case.

That is the reason why the extended parser fails to parse some valid expressions such as \lstinline{(\x.x)} \lstinline{(\y.y)}. To resolve this issue, we need to let \lstinline{pApp} be aware of the change of grammar. We have some direct solutions:

\begin{itemize}
    \item Keep the old code, rewrite problematic functions like \lstinline{pApp}. \\Separate compilation is obtained but code reuse is lost, because every function may contain such recursive calls.
    \item Modify the old code, correct the recursive calls by proper functions. \\We can avoid code duplication thus get code reuse, but separate compilation is lost because of modification on the old code.
    \item ??? \huang{third one? use dynamic languages?}
\end{itemize}

Similar to the trade-off in the Expression Problem, neither of them satisfies us. The challenge of building semantically modular parsers requires fundamental change of the parser structure. We will introduce our solution in the next sections.

\subsection{Delegation-Based Parsing}\label{subsec:overview-firstlook}

Hard-coded recursive calls prevent our parsers from extensibility.
This problem can be solved using delegation, which can be encoded with \textit{open recursion}. The idea is
simple: instead of making direct recursive calls, an additional
argument, that abstracts over the recursive call, is passed to the
parsers. This technique enables the developement of parsers that
support OO-style extensibility of ASTs.

\paragraph{Open Parsing} Consider the expression language of only variables and applications in the previous section. The parser can be rewritten as follows:

\begin{lstlisting}
type Open[T] = (=> T) => T

// Expr, Var and App are the same as previous
trait BaseParser {
  lexical.delimiters += ("(", ")")
  val pVar: Open[PackratParser[Expr]] =
    self => ident ^^ { x => new Var(x) }
  val pApp: Open[PackratParser[Expr]] =
    self => self ~ self ^^ { case e1 ~ e2 => new App(e1, e2) }
  val pExpr: Open[PackratParser[Expr]] =
    self => pVar(self) ||| pApp(self) ||| "(" ~> self <~ ")"
}
\end{lstlisting}

According to the definition of type synonym \lstinline{Open[T]}, every parse function takes a parser of \lstinline{Expr} as a parameter, namely \lstinline{self}. This parameter \lstinline{self} represents \emph{the parser} of expressions, which is dynamiclly explained by the argument. Using an abstract parameter provides extensibility to parsers, because the recursive calls are not hard-coded here --- they are dynamic and can be changed corresponding to the grammar!

We take \lstinline{pApp} as an example again. It uses \lstinline{self} instead of hard-coded \lstinline{pExpr} for recursive parsing. Depending on the real argument, \lstinline{self} can be a parser which covers only variables and applications, or a parser which also supports the lambda case.

\paragraph{Client Code} Since the type of parsing functions are changed, we can not apply them directly. The client code below shows how to use the new parser. The \lstinline{fix} function, which is the the standard \textit{fixpoint combinator}, is used to `close' the open recursion.

\begin{lstlisting}
def fix[T](f: Open[T]): T = { lazy val a: T = f(a); a }

def parse[E](p: Open[PackratParser[E]])(inp: String): E = {
  val t = phrase(fix(p))(new lexical.Scanner(inp))
  if (t.successful) t.get else scala.sys.error(t.toString)
}

println(parse(new BaseParser {}.pExpr)("x y"))
\end{lstlisting}

\bruno{You are copying and pasting code into the paper. Don't do this:
code will change alot (various edits), and it is all too easy to get
inconsistent code snippets. Instead use the small Ruby tool that we
have in the paper directory to import source code directly from the
code repository. This will ensure the consistency of the code
throughout the paper writing. Please ask Haoyuan about the tool; he
knows how to use it!}

\huang{done}\bruno{Please polish up code. You probably want to consistently use
the variable name ``self'' whenever you want to refer to the whole
parser (instead of ``e'' that you use at the moment). Also you should
use ``Open'' (which you currently call ``Fix'') instead of using the
longwinded ``(=> PackratParser[Expr]) => PackratParser[Expr]''}

\huang{done}\bruno{Also, for consistency, why isn't ``pVar'' open? I.e. using a
  self reference as well? I know that it is not recursive, but giving
  it a different treatment makes it more confusing for users.}

\huang{done}\bruno{You should use a more OO approach in the example.
Rather than case classes show an Expr trait with pretty printing.
This will enable you to talk about the extensibility and object
algebras better later on.
}


\paragraph{Extensibility} As same as what we did in previous section, we extend the grammar by adding lambda abstractions as a new case. Similarly, the new parser extends from the old one by adding \lstinline{pLam} on it. Notice that the old code is not touched, hence separate compilation is obtained.

\begin{lstlisting}
trait ExtParser extends BaseParser {
  lexical.delimiters += ("\\", ".")
  val pLam: Open[PackratParser[Expr]] =
    self => "\\" ~> ident ~ ("." ~> self) ^^ { case x ~ b => new Lam(x, b) }
  val pExtExpr: Open[PackratParser[Expr]] =
    self => pLam(self) ||| pExpr(self)
}
\end{lstlisting}

All recursive calls of parsing `an expression' are called via the explicit parameter \lstinline{self}. Once the \lstinline{fix} function is applied, these calls will be updated appropriately. Parsing functions such as \lstinline{pApp} do not need to be rewritten. As a consequence, this parser recognizes all valid expressions of the new grammar, including \lstinline{(\\x.x)} \lstinline{(\\y.y)} which cannot be parsed in the previous section.

\begin{lstlisting}
println(parse(new ExtParser {}.pExtExpr)("(\\x.x) (\\y.y)"))
\end{lstlisting}

As we demonstrated, delegation encoded by open recursion is the key technique to obtain semantic modularity. It enables type-safe code reuse and separate compilation in our parsers. We will discuss this topic further in section \ref{sec:openandparsing}.

\subsection{Object Algebras for full Extensibility}\label{subsec:overview-oa}

Although delegation enables OO extensibility, the use of an OO class
hierarchy makes the addition of new operations over the AST
problematic. Object Algebras~\cite{} enable us to solve this problem, and
offer high flexibility in the choice of operations to be performed
over the AST.


Instead of define the abstract syntax using Scala's case classes, we
can use \textit{Object Algebras} for more extensibility. Object
Algebras is a technique to solve the \textit{Expression Problem},
providing the possibility of extending both data variants and
operations over them. It will be further discussed in section
\ref{sec:algebrasandparsing}.

Using Object Algebras, the abstract syntax of the language of variables and applications is defined as below.

\begin{lstlisting}
trait Alg[E] {
  def Var(x: String): E
  def App(e1: E, e2: E): E
}
\end{lstlisting}

Then we are able to define operations over the syntax in an extensible way. Here we write a pretty printing operation for this language.

\begin{lstlisting}
trait Print extends Alg[String] {
  def Var(x: String) = x
  def App(e1: String, e2: String) = "(" + e1 + " " + e2 + ")"
}
\end{lstlisting}

And also parser for it as below. Notice the parsing function \lstinline{pVarApp} takes an argument of type \lstinline{Alg[E]}, that means it accepts any instance of the algebra interface \lstinline{Alg}.

\begin{lstlisting}
trait Parser[E] {
  lexical.delimiters += ("(", ")")

  val pVarApp: Alg[E] => Fix[PackratParser[E]] = alg => e =>
    e ~ e ^^ { case e1 ~ e2 => alg.App(e1, e2) } | ident ^^ alg.Var | "(" ~> e <~ ")"
}
\end{lstlisting}

We glue them together by defining an object. It actually becomes a \textit{language component} that represents a specific language feature.

\begin{lstlisting}
object VarApp {
  trait Alg[E] {..}
  trait Print extends Alg[String] {..}
  trait Parser[E] {..}
}
\end{lstlisting}

To use the parser, we must provide an algebra instance as the operation to construct the parsing result. As shown in the code below, we feed the pretty printing operation \lstinline{VarApp.Print} to the generic \lstinline{parse} function. It will print \lstinline{(x y)} as the result.

\begin{lstlisting}
def parseWithAlg[E](inp: String)(alg: VarApp.Alg[E]): E = {
  val p = new VarApp.Parser[E] {}.pVarApp(alg)
  parse[E](p)(inp)
}

println(parseWithAlg("x y")(new VarApp.Print{}))
\end{lstlisting}

With Object Algebras, we can extend all the stuff, including abstract syntax, operations, and parsers. The code below extend the language to support lambda abstraction.

\begin{lstlisting}
object Lambda {
  trait Alg[E] extends VarApp.Alg[E] {
    def Lam(x: String, e: E): E
  }

  trait Print extends Alg[String] with VarApp.Print {
    def Lam(x: String, e: String) = "\\" + x + "." + e
  }

  trait Parser[E] extends VarApp.Parser[E] {
    lexical.delimiters += ("\\", ".")

    val pLam: Alg[E] => Fix[PackratParser[E]] = alg => e =>
      ("\\" ~> lcid) ~ ("." ~> e) ^^ { case x ~ e0 => alg.Lam(x, e0) }
    val pVarAppLam: Alg[E] => Fix[PackratParser[E]] =
      pVarApp | pLam
  }
}
\end{lstlisting}

For easily combining the parsers using Object Algebras as the representation of abstract syntax, we have a special combinator \huang{we need to rename our combinator} defined in our library.

In addition to make extending operations possible, another advantage of using Object Algebras is that it supports multiple sorts of syntax. For example, if our language has not only expressions but also types, we can easily distinguish them in the abstract syntax by an extra type parameter. This topic will be discussed in section \ref{subsec:differentsyntax}.

\begin{lstlisting}
trait TypedLambda[E, T] extends VarApp.Alg[E] {
  def Lam(x: String, t: T, e: E): E
  def IntType(): T
  def BoolType(): T
  def ArrowType(a: T, b: T): T
}
\end{lstlisting}



% \begin{itemize}
% \item Choosing the Parsing Technology
%     \begin{itemize}
%     \item Parser Generators : why not?
%         \begin{itemize}
%             \item Not type-safe
%             \item No modular type-checking
%             \item Not modular or no separate compilation (but we need to mention lots of work on extensible parsing here: example Language Workbenches; Rats)
%         \end{itemize}
%     \item Parser Combinators
%         \begin{itemize}
%         \item Backtracking parsers (Parsec)
%             \begin{itemize}
%             \item Need to remove left recursion (Problem: Transformation is not modular; if we do not know the full grammar then cannot be done)
%             \item Need try/backtracking (Problem: Since we do not know the full set of rules in a modular setting, we have to assume worst case scenario and add redundant backtracking. )
%             \item Mention some possible workarounds (there may be some but they still have issues)
%             \end{itemize}
%         \item (Non)-Backtracking/Packrat Parsers (Works well in a modular setting) (find a good name for this type of parsers?)
%         \end{itemize}
%     \end{itemize}
% \item Adapting Parser Combinators for modularity
%     \begin{itemize}
%     \item Using Object Algebras
%     \item Using Open Recursion
%     \item Using new modular parser combinators (Library: new alternative combinator, for example)
%     \end{itemize}
% \item Small example
%     \begin{itemize}
%     \item small example of a modular parser using our technique
%     \item show also the same example without modularity and write a detailed comparison
%     \item Lambda Calculus is a good candidate for the example
%     \item Show a small extension (adding plus and numeric literals)
%     \end{itemize}
% \end{itemize}
