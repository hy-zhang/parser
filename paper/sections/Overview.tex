\section{To be discussed in Introduction}

\haoyuan{I suggest this part to be moved to Introduction, and only discuss why Packrat is selected among
different parser combinators in Overview.}

Although there are many parsing techniques, not all of them are
suitable for type-safe modular parsing. In particular there are many
techniques which fail to provide modular type-checking and separate
compilation. Moreover, even if modular type-checking and separate
compilation are supported, efficiency is another
concern. A parsing technique should have low overhead when applied
in a modular setting. In the remaider of this section, we eliminate
various techniques that fail to satisfy our requirements, and argue
that that Packrat parsing~\cite{Ford2002} is a suitable candidate for
type-safe modular parsing.

\paragraph{Parser Generators} The most widely use tools for parsing
are parser generators. Parser generators help users generate parsers automatically or
semi-automatically from a given grammar. There is no restriction on
the algorithm, while most of them adopts table-based LL~\cite{lewis1968syntax} and LR~\cite{knuth1965translation} parsing
algorithms.
Although efficient, the main drawback of parser generators is that they do not support
modular type-checking and separate compilation.

Modular parsing based on parser generators is supported by many libraries~\cite{antlr1995,Grimm2006,Gouseti2014,Warth2016}. Users can separate the syntax definition and related parsing code into reusable components. Then the corresponding parsers are built by their library utilities. For example, NOA~\cite{Gouseti2014} uses Java annotation processing to collect grammar information, and then generates ANTLR4 parsers. However, such generation procedure requires a whole compilation after the collection of all grammar pieces. Once the grammar changed, even slightly, grammar information must be re-collected and the parser must be re-generated. Hence those libraries only have syntactic modularity.

Generating parsers often requires full information of grammars, thus semantic modularity is difficult to achieve in this way.

\paragraph{Parser Combinators}
Comparing with the parser generators, a \textit{parser combinator}~\cite{burge1975,Wadler1985}
takes several parsers and produce a new parser as its output. Parser combinators are
popular in functional programming, where the parsers are represented
by functions and parser combinators are higher-order functions accepts
them.

At a first look, parser combinators are very suitable for our purpose, because of two
reasons. Firstly, they are naturally modular. The manner of using them
is to write small parsers and use combinators to composed them
together. The construction procedure is explicit and fully controlled
by the programmer. Secondly, each parser combinator is represented by
a piece of code, and also are the parsers it takes. Thus in a
statically typed programming language they can be statically
type-checked.

\section{Overview}\label{sec:overview}

This section gives an overview of our library \name, and the problem that motivates our work. Basically, \name consists of four parts: underlying parsing technique, delegation mechanism encoded by open recursion, Object Algebras, and glue code of new combinators and utility functions. We start from Section \ref{subsec:overview-parsing}, which discusses the choice of parsing technique and how it affects modularity of parsers. Section \ref{subsec:overview-problem} demonstrates the goal of extending parsers together with ASTs in a semantic modular way, with both separate compilation and type-safe code reuse. Then we will see traditional parser combinators fail to achieve it because of hard-coded recursive calls. In Section \ref{subsec:overview-delegation}, we show how delegation can solve this problem and allow us to build extensible parsers. Finally, Section \ref{subsec:overview-oa} gives examples of using Object Algebras for more extensibility, including extension of operations and parsing multiple sorts of syntax.\haoyuan{TODO}

\subsection{Choosing the Parsing Technique}\label{subsec:overview-parsing}

%A technique for type-safe modular parsing should the following 3
%features: \emph{modular type-checking}; \emph{separate compilation};
%low performance overhead

In the last section, we have argued that parser combinators are a suitable parsing technique for
our purpose, as they naturally build modular parsers for type-checking.
Unfortunately many parser combinators have important limitations.
In particular several parser combinators,
including the famous Parsec~\cite{Leijen2001} library, require
programmers to manually do \textit{left-recursion elimination} and \textit{longest match composition}, and
require significant amounts of \textit{backtracking}. All of them are
problematic in a modular setting.



\paragraph{Left-Recursion Elimination} The top-down, recursive descent parsing strategy adopted by those parser combinator libraries cannot support left-recursive grammars directly. The common solution is to rewrite the grammar into an equivalent but not left-recursive one, so called left-recursion elimination.

A left-recursive grammar of adding integers can be rewritten as below. On the right side, we got a new grammar without left-recursion.

\begin{tabular}{m{0.4\linewidth}m{0.4\linewidth}}
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> \alt <expr> `+' <int>
\end{grammar}
&
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> <expr'>

<expr'> ::= <empty> \alt `+' <int> <expr'>
\end{grammar}
\end{tabular}

The main problem with left-recursion elimination is that it is a
\emph{global} transformation on a grammar. Given that the full grammar
is known, then it is possible to remove all left-recursive
cases. However, when doing modular parsing, the full grammar is not
known!

For the example above, if we extend the original grammar to support
subtraction, we must analyse the full grammar again to rewrite it.

\begin{tabular}{m{0.4\linewidth}m{0.4\linewidth}}
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> \alt <expr> `+' <int> \alt <expr> `-' <int>
\end{grammar}
&
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> <expr'>

<expr'> ::= <empty> \alt `+' <int> <expr'> \alt `-' <int> <expr'>
\end{grammar}
\end{tabular}


Another issue of left-recursion elimination is that it requires extra
bookkeeping work to retain the original semantics. For example, the
expression $1+2-3$ is parsed as $(1+2)-3$ in the left-recursive
grammar, but after rewrite the result is $1((+2)-3))$. The parse tree
must be transformed to recover its structure.

\huang{todo}
\bruno{This example does not appear to be problematic: adding ``-''
  does not affect the rest of the grammar. Isn't the problem that
  sometimes adding a new case to the grammar, requires changing other
parts? Otherwise I don't see why left-recursion is a problem, and we
cannot simply claim that it is a problem. You have to make a better
effort to find a problematic grammar.
}

\paragraph{Longest Match Composition}
Some parsing libraries provide combinators for alternative composition with longest match,
while others do not. Now consider the non-left-recursive grammar:
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int> \alt <int> `+' <expr>
\end{grammar}
In Parsec, for instance, the parser
\begin{lstlisting}[language=PlainCode]
parser = parseInt <|> parseAdd
\end{lstlisting}
will only parse the input \inlinecode{"1 + 2"} to \inlinecode{"1"}, as \inlinecode{parseInt} successfully parses \inlinecode{"1"}
and terminates parsing. With traditional alternative
composition in recursive descent parsers, the parser in the front successfully parses a substring of the input, then it unexpectedly
finishes parsing and discards the rest, in spite that latter parsers might be able to parse the whole input.
In contrast, \inlinecode{parser = parseAdd <|> parseInt} works as expected, yet manual composition for longest match is quite
inconvenient, especially in a modular setting. When the grammar is extended with new rules, programmers are supposed to adjust
the order of parsers, which is tedious.

\paragraph{Backtracking} The need for backtracking is also problematic
in a modular setting. For example, suppose that we only have \inlinecode{import..from} statements before,
but now we want to extend the grammar with the \inlinecode{import..as} case, as shown in the third line of grammar below.

\setlength{\grammarindent}{5em}
\begin{grammar}
<stmt> ::= `import' <ident> `from' <ident>
    \alt ...
    \alt `import' <ident> `as' <ident>
\end{grammar}

Since the \inlinecode{import..from} case shares a prefix with the new case \inlinecode{import..as}, when the former case fails, we must backtrack to the beginning. Take Parsec as an example, its choice
combinator \inlinecode{<|>} only tries the second alternative if the first fails
without any token consumption. An auxiliary function \inlinecode{try} is used for explicit backtracking.

\begin{lstlisting}[language=PlainCode]
oldParser = parseImpFrom <|> parseA <|> parseB <|> ...
newParser = try parseImpFrom <|> parseA <|> parseB <|> ... <|> parseImpAs
\end{lstlisting}

Given the full grammar, we can decide where to put \inlinecode{try} for backtracking. However, with modular parsing we are unable to overlook the full grammar, hence the worst case should be considered, namely all alternatives may share common prefixes with future cases. In that case we need to backtrack for all the branches.
To avoid failures in the future, we have to add \inlinecode{try} everywhere:

\begin{lstlisting}[language=PlainCode]
parser = try parseImpFrom <|> try parseA <|> try parseB <|> ... <|> try parseImpAs
\end{lstlisting}
which results in the worst exponential time complexity.

%\huang{I've rewritten this paragraph}\bruno{The example is good, I think but the explanation is not.
%You want to say that when you know
%  the full grammar, you can figure out where ``try'' is
%  needed. Without the full grammar you'd need assume the worst
%  case. Make an effort to make your explanation cristal clear!
%I think you want to miss the abstract explanation that you give first,
%with the explanation about the concrete example.
%Start with ``The need for backtracking is also problematic
%in a modular setting. For example, suppose ...'' and synchronize
%the abstract explanation and the explanation for the example.
%}

\paragraph{Packrat Parsing}
Fortunately some more advanced parsing techniques such as Packrat
parsing~\cite{Ford2002}, address the limitations of simple parser combinators
such as Parsec. Packrat parsers use
memoization to record the result of applying each parser at each
position of the input, so that repeated computation is eliminated.
Moreover, theoretically the algorithm behind Packrat parsers
supports both direct and indirect left-recursion~\cite{warth2008}.
The current version of the library is still
buggy with indirect left-recursion, but we believe that
they will fix it in the future, and direct left-recursion is
already practical to use for a large number of applications. All of these properties are very
suitable for modularity, thus we decided to use Packrat parsers as the underlying
parsing technique in \name.


It is worth mentioning that the choice of parser combinators will not
affect the other parts of our library. One can choose other parser
combinators like Parsec, in cases that the performance and supporting
of left-recursion are not major concerns. A different library can even build a new
\name with fancy features or higher efficiency.

\subsection{Packrat Parsing and Our Library \name}\label{subsec:overview-library-api}

Our prototype library is implemented in Scala, with its foundation on Packrat Parsing. In the current version of Scala (2.11), Packrat Parsing is included
in the Scala standard parser combinator library, which is an external jar file. The library provides a couple of
parser combinators, together with some basic parsers. Below we present an example to illustrate Scala parsers.

\paragraph{An Example}
A Packrat parser has type \lstinline{PackratParser[E]} for some
\lstinline{E}, which indicates the type of results it produces.
%\bruno{the representation of what?}
And we use \lstinline{lexical} for its lexing.

\lstinputlisting[linerange=8-12]{../Scala/Parser/src/PaperCode/SampleParser.scala}% APPLY:linerange=PACKRAT_EXAMPLE
\noindent
Note that the code should be defined in an enclosing type that extends
\lstinline{scala.util.parsing.combinator.syntactical.StandardTokenParsers}
for lexing and
\lstinline{scala.util.parsing.combinator.PackratParsers}.
The first two lines are used for lexing. The third line defines a
parser which can, for instance, parse \lstinline{"str(5)"} and produce
the integer \lstinline{5}, where \lstinline{numericLit} is just a basic parser
in the API. Table~\ref{tab:packrat} gives a closer look at part of the API, which covers our experiments.

\begin{table}[t]
\begin{tabular}{l}
\hline
\begin{lstlisting}
def ~[U](q: => Parser[U]): Parser[~[T, U]]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator for sequential composition. \\
\hline
\begin{lstlisting}
def ^^[U](f: (T) => U): Parser[U]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator for function application. \\
\hline
\begin{lstlisting}
def ^^^[U](v: => U): Parser[U]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator that changes a successful result into the specified value. \\
\hline
\begin{lstlisting}
def <~[U](q: => Parser[U]): Parser[T]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator for sequential composition which keeps only the left result. \\
\hline
\begin{lstlisting}
def ~>[U](q: => Parser[U]): Parser[U]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator for sequential composition which keeps only the right result. \\
\hline
\begin{lstlisting}
def repsep[T](p: => Parser[T], q: => Parser[Any]): Parser[List[T]]
\end{lstlisting} \\
\hspace{.2in}- A parser generator for interleaved repetitions. \\
\hline
\begin{lstlisting}
def ident: Parser[String]
\end{lstlisting} \\
\hspace{.2in}- A parser which matches an identifier. \\
\hline
\begin{lstlisting}
def numericLit: Parser[String]
\end{lstlisting} \\
\hspace{.2in}- A parser which matches a numeric literal. \\
\hline
\begin{lstlisting}
def |[U >: T](q: => Parser[U]): Parser[U]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator for alternative composition. \\
\hline
\begin{lstlisting}
def |||[U >: T](q0: => Parser[U]): Parser[U]
\end{lstlisting} \\
\hspace{.2in}- A parser combinator for alternative with longest match composition. \\
\hline \\
\end{tabular}
\caption{Part of Scala Parser API.\bruno{We should double-check that this table lists the
  combinators used in the paper.}}\label{tab:packrat}
\end{table}

\begin{table}[t]
\begin{tabular}{l}
\hline
\begin{lstlisting}
type OpenParser[Alg, R, E] = Alg => (=> R) => PackratParser[E]
\end{lstlisting} \\
\hspace{.2in}- The type constructor for modular parsers. \\
\hline
\begin{lstlisting}
def parse(p: Parser[_]): String => Unit
\end{lstlisting} \\
\hspace{.2in}- A function that applies a parser to an input, and prints the result as a string. \\
\hline
\begin{lstlisting}
implicit def <|>(x: OpenParser[Alg1, R, E], y: OpenParser[Alg2, R, E])
    : OpenParser[Alg1 with Alg2, R, E]
\end{lstlisting} \\
\hspace{.2in}- Open parser combinator for alternative with longest match composition. \\
\hline \\
\end{tabular}
\caption{Our parser combinator library \name API.\haoyuan{I don't know if I can present <|> like this.}}\label{tab:parser}
\end{table}

Finally, a generic \inlinecode{parse}
function is used for testing:

\lstinputlisting[linerange=16-19]{../Scala/Parser/src/PaperCode/SampleParser.scala}% APPLY:linerange=PACKRAT_RUNPARSER
Now \lstinline{parse(p)("str(5)")} produces \lstinline{5} as expected.

\paragraph{\name} Our library is constructed by only around twenty lines of code. We have wrapped up the type of parsers
to make them modular and extensible, and we use our custom combinator \lstinline{<|>} for alternative composition of those
modular ``open parsers'' with longest match, built on \lstinline{|||}. Table~\ref{tab:parser} shows the API that \name provides for modular parsing.

\subsection{The Parsing Expression Problem}\label{subsec:overview-problem}
%\huang{done, added the attempt/failure/reason of extending conventional parsers}\bruno{You are not motivating the problem! You are going straight to
%  the solution without pointing out what the problem is first. What
%  you need to do is: First show what happens with conventional
%  parsers: at some point, if you add extensions the recursive calls
%  will be wrong. Then you show (in the next section) the solution:
% use delegation/open recursion.}

%\huang{partly done, only extend a new language construct in the example, how about leaving extension of new operations in the OA subsection?}\bruno{I think we need to set up a challenge here, similar to the challenge of
%  the expression problem. The challenge should be like. Build a parser
%for a simple expression language, then extend the language with both
%a new language construct and a new operation. This section will show
%how we can do that using traditional parsers, but without modular
%type-safety and separate compilation. The remaining sections will show
%that the techniques introduced by us, enable us to solve those two challenges.}

In the following text we are going to illustrate the problem that motivates our work.
Typical parsers defined by parser combinators follow the grammar
structure, they parse an input to build a corresponding AST. If the grammar is extended
with a new alternative, the corresponding parser has to be changed as
well. Thus the traditional style of developing parsers is not
extensible, and it is at odds with object-oriented programming ASTs,
which support the easy addition of new alternative classes. We
illustrate this with a simple expression language and extensions.

Suppose we want to parse a simple language of literals and
additions. The concrete syntax is as follows:

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <int>
    \alt <expr> `+' <expr>
\end{grammar}
%\alt `(' <expr> `)'

This time we allow both parts of an addition to be expressions.
It is straightforward to model ASTs by inheritance and write corresponding parsers for all cases:
%\huang{This is the first time to demonstrate code in this paper. Readers may not know what those combinators/types are. Refer to the library docs here? move the 'scala parsing library' section to the beginning as 'background'?}

\lstinputlisting[linerange=8-24]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_SIMPLE_EXPR

\noindent Note that we use \lstinline{Parser} as a type synonym for \lstinline{PackratParser} in the rest of paper. As the combination of \inlinecode{pVar}, \inlinecode{pApp}, \inlinecode{pExpr} can parse expressions like \lstinline{"1+2"} correctly. At this point, one would like such code to be extensible in two dimensions, for example:
\begin{itemize}
\item \textbf{Challenge 1}: adding a new case (or rather, a new construct) to \lstinline{Expr};
\item \textbf{Challenge 2}: adding a new operation to \lstinline{Expr}.
\end{itemize}

\paragraph{Attempt to Extend the Parser} Now we firstly consider challenge 1, namely we are extending the syntax as well as the parser. Hence we introduce variables as a new case.

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= ...
   \alt <ident>
\end{grammar}

The corresponding AST is extended as below.

\lstinputlisting[linerange=28-30]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_SIMPLE_LAM

Since we already have the parser for literals and additions, we would like to build the new parser by reusing the old one. So we compose \inlinecode{pVar}, which parses variables, with the old parser. \inlinecode{pExtExpr} is the new function to parse expressions.

\lstinputlisting[linerange=34-37]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_SIMPLE_EXT

Unfortunately, this extended parser does not work properly to parse all possible expressions defined by the new grammar. For example, it fails to parse \inlinecode{"1 + x"} which is obviously valid.

\paragraph{Problem: Hard-coded Recursive Calls} The critical problem of extending parsers directly is that the recursive calls in old parsers are hard-coded. Specifically, \inlinecode{pAdd} makes two recursive calls to parse two sub-expressions around \lstinline{"+"}. Before the extension, we implemented it by using \inlinecode{pExpr} for those recursive calls. It worked fine at that time because \inlinecode{pExpr} covers all cases of the old grammar. However, after the extension of variables, the proper function for such recursive calls should be \inlinecode{pExtExpr} instead of \inlinecode{pExpr}, because the new function \inlinecode{pExtExpr} includes the new extension.

That is the reason why the extended parser fails to parse some valid expressions such as \inlinecode{"1 + x"}. To resolve this issue, \inlinecode{pAdd} needs to be aware of the change of grammar. There are some possible solutions:

\begin{itemize}
    \item Keep the old code, rewrite the non-extensible function \inlinecode{pAdd}. \\Separate compilation is obtained but code reuse is lost, because every function may contain such recursive calls.
    \item Modify the old code, correct the recursive calls by proper functions. \\We can avoid duplication for code reuse, but separate compilation is sacrificed because of modification on the existing code.
    \item ??? \huang{third one? use dynamic languages?}
\end{itemize}

Because of the trade-off, neither of them are satisfying. The challenge of building semantically modular parsers requires fundamental change of the parser structure. We will introduce our solution in the next sections.

\subsection{Delegation-Based Parsing}\label{subsec:overview-delegation}

Hard-coded recursive calls prevent parsers from extensions.
This problem can be solved using delegation, which can be encoded with
\textit{open recursion}. The idea is simple: instead of making direct
recursive calls, an additional argument, that abstracts over the
recursive call, is passed to the parsers. This technique enables the
development of parsers that support OO-style extensibility of ASTs.

\paragraph{Open Parsing} Consider the expression language of only literals and additions in the previous section. The parser can be rewritten as follows:

\lstinputlisting[linerange=58-68]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_BASE

According to the definition of type synonym \inlinecode{Open[T]},
every parsing function takes a parser of \inlinecode{Expr} as a
parameter, namely \inlinecode{self}. This parameter \inlinecode{self}
represents \emph{the parser} of expressions, which is dynamiclly
explained by the argument. Using such an abstract ``self-reference'' provides
extensibility to parsers, because the recursive calls are not
hard-coded here --- they are dynamic and can be changed corresponding
to the grammar!
We take \inlinecode{pAdd} as an example again. It uses
\inlinecode{self} instead of hard-coded \inlinecode{pExpr} for
recursive parsing. Depending on the real argument, \inlinecode{self}
can be a parser which covers only literals and additions, or a
parser which also supports variables.

\paragraph{Client Code} Since the type of parsing functions is
changed, we can not apply them directly. A \inlinecode{fix} function, which is the
the standard \textit{fixpoint combinator}, is used to `close' the open
recursion. After that, it can be used as an ordinary parser.
The client code below demonstrates the usage of new parsers.

%\huang{done}\bruno{Too much detail is being given here about open
%  recursion. In Section 2 we should talk only about \emph{how} to use
%open recursion, and not how open recursion is implemented. We
%can summarize, for example the API related to open recursion.
%Section 4 is where the implementation and detailed explanation
%about open recursion should come.
%}

\lstinputlisting[linerange=73-77]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_USE

%\huang{done}\bruno{Please polish up code. You probably want to consistently use
%the variable name ``self'' whenever you want to refer to the whole
%parser (instead of ``e'' that you use at the moment). Also you should
%use ``Open'' (which you currently call ``Fix'') instead of using the
%longwinded ``(=> PackratParser[Expr]) => PackratParser[Expr]''}
%
%\huang{done}\bruno{Also, for consistency, why isn't ``pVar'' open? I.e. using a
%  self reference as well? I know that it is not recursive, but giving
%  it a different treatment makes it more confusing for users.}
%
%\huang{done}\bruno{You should use a more OO approach in the example.
%Rather than case classes show an Expr trait with pretty printing.
%This will enable you to talk about the extensibility and object
%algebras better later on.
%}

\paragraph{Extensibility} Similarly, we extend the grammar by adding lambda abstractions as a new case. The new parser also extends the old one by adding \inlinecode{pLam} on it. Notice that the old code is not touched, hence separate compilation is obtained.

\lstinputlisting[linerange=82-88]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_EXT

\haoyuan{Code without combinators?}

All recursive calls of parsing `an expression' are called via the explicit parameter \inlinecode{self}. Once the \inlinecode{fix} function is applied, these calls will be updated appropriately. Parsing functions such as \inlinecode{pApp} do not need to be rewritten. As a consequence, this parser recognizes all valid expressions of the new grammar, including \inlinecode{(\\x.x)} \inlinecode{(\\y.y)} which cannot be parsed in the previous section.

\lstinputlisting[linerange=94-94]{../Scala/Parser/src/PaperCode/Overview/Overview.scala}% APPLY:linerange=OVERVIEW_OPEN_EXT_USE

As we demonstrated, delegation encoded by open recursion is the key technique to obtain semantic modularity. It enables type-safe code reuse and separate compilation in our parsers. We will discuss this topic further in Section \ref{sec:openandparsing}.

\subsection{Object Algebras for full Extensibility}\label{subsec:overview-oa}

Although delegation enables OO extensibility, the use of an OO class
hierarchy makes the addition of new operations over the AST
problematic. Object Algebras~\cite{Oliveira2012} enable us to solve this problem, and
offer high flexibility in the choice of operations to be performed
over the AST. It also makes parsing with multiple sorts of syntax easier.

\paragraph{Parsing with Object Algebras} Using Object Algebras, the abstract syntax of the language of variables and applications is defined as below.

\lstinputlisting[linerange=8-11]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_ALG

Then we are able to define operations over the syntax in an extensible way. Here we write a pretty printing operation for this language.

\lstinputlisting[linerange=15-18]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_PRINT

And also parser for it as below. Notice the parsing function \inlinecode{pExpr} takes an argument of type \inlinecode{ExprAlg[E]}, that means it accepts any instance of the algebra interface \inlinecode{ExprAlg}.

\lstinputlisting[linerange=22-28]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_PARSER

\paragraph{Extensibility of Syntax} Following previous examples, the code below shows how to extend the language to support lambda abstractions.

\lstinputlisting[linerange=32-46]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_EXT

For easily combining parsers using Object Algebras as ASTs, we have a special combinator \huang{we need to rename our combinator} defined in our library.

To use the parser, we must provide an algebra instance as the operation to construct the parsing results. In the code below, we use the pretty printing operation \inlinecode{LambdaPrint} as the algebra instance, so that the parsing result is a pretty printed AST.

\lstinputlisting[linerange=55-60]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_USE

\paragraph{Extensibility of Operations} With Object Algebras, operations over ASTs can also be extended in a modular way. Here is an example of collecting free variables from an expression. We can use this operation during parsing and obtain a set of free variables.

\lstinputlisting[linerange=74-80]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_EXT_OP


\paragraph{Multiple Sorts of Syntax} Another advantage of using Object Algebras is that it supports multiple sorts of syntax easily. In several cases, we want to divide the syntactic elements into some groups. For example, the grammar below has two sorts, which are expressions and types.

\begin{tabular}{m{0.4\linewidth}m{0.4\linewidth}}
\setlength{\grammarindent}{5em}
\begin{grammar}
<type> ::= `int' \alt <type> `->' <type>
\end{grammar}
&
\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <ident> \alt <expr> <expr> \alt `\\' <ident> `.' <expr>
\end{grammar}
\end{tabular}

Using Object Algebras, we can easily distinguish them in an extensible way, just by adding an extra type parameter.

\lstinputlisting[linerange=65-69]{../Scala/Parser/src/PaperCode/Overview/OA.scala}% APPLY:linerange=OVERVIEW_OA_MULTI_SYNTAX

