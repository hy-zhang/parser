\section{Related Work}\label{sec:relatedwork}

%- extensible parsing, language workbenches: rats, noa (this one already uses OA), modular semantic actions, (syntactic modularity, no separate compilation, modular type-checking)
%(read more papers, see if they talk about this issue, some potential solutions)
%(attribute grammars?)
%
%- parser combinators for type-checking, previous work has not shown how to support modularity (ASTs); left-recursion and back-tracking in related techniques
%
%- modularity: object algebras, dtc and mrm (problem with parsing, is there any related work? (PB: a paper on unfolds: build the AST))
%
%(parsing in Javascript: using delegation, does it support modular AST)
%
%noa, shy: shy: only override some interesting cases (transformation is tedious)
%bruijn indices: parsing + transformation

Our work integrates several components including extensible parsing, parser combinators and modular datatypes. There has been a great amount of related papers
on those hot topics, of which some inspired us of this paper and encourages us for more exploration. This section will try to lead a discussion on what difference we have made.

\paragraph*{Extensible Parsing} Extensible parsing is achieved in many different areas, of which parser generators are a mainstream area specially designed for modular syntax and parsing. Many parser generators~\cite{antlr1995,Grimm2006,Gouseti2014,Warth2016} support modular grammars, more specifically, they allow users to create new modules where new non-terminals and production rules can be introduced, some can even override existing rules in the old grammar modules. For instance, \textit{Rats!}~\cite{Grimm2006}
constructs its own module system for the collection of grammars, while NOA~\cite{Gouseti2014} uses Java annotation processing to gather information together. Those parser combinators focus on the \textit{syntactic extensibility} of grammars, and rely on a whole compilation to generate a global parser, even with a slight modification. Some of them may statically check the correctness and unambiguity of grammars, but separate compilation and modular type-checking remain unsolved, together with the issue of performance.

Besides, macro systems like C preprocessor, C++ templates and Racket~\cite{Tobin-Hochstadt2011}, and other meta-programming techniques are a similar area aiming at syntactic extensibility as well, which sacrifices type safety. SugarJ~\cite{Erdweg2011} is another well-known tool that conveniently introduces syntactic sugars in Java programming by library imports. Composition of syntactic sugars is easy for users, whereas it requires many rounds of parsing and adaption, which highly affects efficiency of compilation, moreover, the implementation was based on SDF~\cite{Heering1989} and Stratego~\cite{Visser2001}, which focused little on separate compilation. Extensible compilers like JastAdd~\cite{Ekman2007} and Polyglot~\cite{Nystrom2003}, however, are somehow more ambitious, but they involve extensible parsing mostly by parser generators as well, and they focus more on the extensions to a host language. Our library is designed for modular language parsers in a type-safe way, with flexible language composition. Although overriding existing production rules is tricky with our approach, we could perhaps make use of embedded domain-specific languages on top of parser combinators and transformations for overriding, but it is anyway an orthogonal issue. \haoyuan{???} \haoyuan{BTW what about Racket?} \haoyuan{what about metafront? it is a macro system but does it have type safety?} \haoyuan{Extensible syntax with lexical scoping?} \haoyuan{"Extensible syntax" proposes a system for extensible syntax, where users write EDSLs in their language with concrete syntax. Users can write rules for type-based disambiguation. But separate compilation is again not mentioned. Shall we mention that thesis?} \haoyuan{attribute grammars?}

On the other hand, extensible parsing algorithms are another area that indeed relates to separate compilation. Specifically, \cite{Bravenboer2009} introduces \textit{parse table composition}, in which paper grammars are compiled to the generation of parse tables, which are DFAs or NFAs, later they can be composed by an algorithm, so as to provide separate compilation for parsing. Nevertheless, the generation of parse tables is rather expensive, and furthermore, our approach supports separate compilation as well as modular type-checking, and the idea is not restricted to Scala but applicable to many functional languages, on whose type system the safety of modular parsing can rely. The extensibility of parsing in our approach is further available at language composition and lexical level.
\haoyuan{I mentioned one shortcoming of our approach here, which is we cannot override existing production rules; another one is that we do not have explicit correspondence/relationship between abstract syntax and the parser.}

\paragraph*{Parser Combinators} Since~\cite{burge1975} firstly introduced parser generators, and after Wadler discussed more details on backtracking in~\cite{Wadler1985}, parser combinators have been more and more popularly developed and used in the research area of parsing. Among them many parsing libraries produces recursive descent parsers by introducing functional, or more specifically, monadic parser combinators with a foundation on~\cite{nott237}. Related work includes Parsec~\cite{Leijen2001}, which is frequently used in Haskell for context-sensitive grammars with infinite lookahead. Nevertheless, left-recursion is known as a big issue in recursive descent parsers, in which case programmers using libraries like Parsec have to manually eliminate left-recursion in the grammar, which is cumbersome, but also it distorts the shape of its old grammar, restricting the modularity of parsing to extreme extent.
There are certainly some approaches to afford the limitation of parser combinators, for example, we have conducted our previous experiment in Haskell, by using open recursion, MRM~\cite{Oliveira2015} and Parsec. To support direct left-recursive grammars, we made use of the state monad in Parsec to ensure that a left-recursive production rule will not be applied successively in parsing. It turned out to be successful, yet introduced complexity in programming, hence we believe that the parsing library is obligated for left-recursion.

Some more recent papers~\cite{Ford2002,Might2011,Frost2008} \haoyuan{Pracitical, general parser combinators?} proposed a series of novel parsing techniques, moreover, they did put an eye on the issue of left-recursion. As we explained before, we have selected Packrat Parsing for our prototype implementation, as the combinators are convenient to use in Scala, which is a platform where functional laziness and Object Algebras can perfectly coexist. Also, its support for direct left-recursion is already helpful for designing real-world parsers practically, though~\cite{warth2008} further demonstrated that general left-recursion is supported from the theoretical point of view. We have also mentioned that the components used in our library can have alternatives, for instance, a different set of parser combinators may support ambiguous and left-recursive grammars, or be applicable to a different subset of context-free grammars or more, or even lead us to another level of performance.

Another important issue in previous papers, to the best of our knowledge, is that none of them deal with modular datatypes or abstract syntax trees. The modularity of data structures is yet necessary for modular parsing that supports separate compilation and modular type-checking. In our library, Object Algebras have been adopted for extensible ASTs, together with the behaviors on them.

\paragraph*{Modular Datatypes} There has been lots of related work on building modular data structures, especially in some functional languages with non-extensible datatypes like Haskell. For instance, the famous DTC paper~\cite{swierstra2008} represents modular ASTs using co-products of every two functors, which forms a binary tree structure. It supports extensible datatypes, as well as the operations on them by implementing instances of functors. Later Oliveira et al. presented MRM~\cite{Oliveira2015} in Haskell, which is similar to DTC but adopts a list of functors in type-level lists instead. As mentioned above, we have conducted our early experiment with MRM for modular datatypes, whereas the complexity of library implementation was comparably high. Other related work includes~\cite{Bahr2014}, where data types are flexible with composition as well as decomposition.

In OO languages, however, things get much better with the mechanism of inheritance. Furthermore, Object Algebras~\cite{Oliveira2012} were proposed as a design pattern for the extensibility of both data variants and operations in a modular type-safe way. The code is concise and expressive using Object Algebras, building a foundation for our new library with various dimensions of extensibility. Moreover, in~\cite{Oliveira2012} the authors have discussed the composition of algebras. In our parsing approach, a parser consumes an algebra, which is delegated to return the results, during its process of parsing. Having a set of algebras, it requires multiple parsing with several times of invocation, which leads to redundant work. Instead, algebras are supposed to be composed into one before the invocation of the parser. Bahr et al. lead a similar discussion in~\cite{Bahr2011}, where queries (or \textit{catamorphisms}) and transformations (or \textit{homomorphisms}) are composable. They have also mentioned the dual process of folds, namely \textit{anamorphisms}. It is potentially related to our work, as parsing is a representative kind of unfolds, whereas they only discussed the composition of a cv-coalgebra and a term homomorphism, which differs from modular parsing.
\haoyuan{From Object Algebras to Attribute Grammars?}

Another interesting observation, as we have mentioned in~\ref{subsec:objectalgebras}, is that transformation algebras are frequently used in parsing, as concrete syntax may not always coincide with its abstract syntax, for instance, the bruijn indices in lambda calculus, as well as other syntactic sugars. In the Shy framework~\cite{Zhang2015}, the pattern of transformations is captured into a template for Java annotation processing, and most of boilerplate code is automatically generated, hence users only need to override a few interesting cases, conveniently. The idea of overriding existing cases was also proposed in MRM. It is potentially useful to our framework.
