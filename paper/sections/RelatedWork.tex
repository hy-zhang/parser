\section{Related Work}\label{sec:relatedwork}

- extensible parsing, language workbenches: rats, noa (this one already uses OA), modular semantic actions, (syntactic modularity, no separate compilation, modular type-checking)
(read more papers, see if they talk about this issue, some potential solutions)
(attribute grammars?)

- parser combinators for type-checking, previous work has not shown how to support modularity (ASTs); left-recursion and back-tracking in related techniques

- modularity: object algebras, dtc and mrm (problem with parsing, is there any related work? (PB: a paper on unfolds: build the AST))

(parsing in Javascript: using delegation, does it support modular AST)

noa, shy: shy: only override some interesting cases (transformation is tedious)
bruijn indices: parsing + transformation

Our work integrates several components including extensible parsing, parser combinators and modular datatypes. There has been a great amount of related papers
on those hot topics, of which some inspired us of this paper and encourages us for more exploration. This section will try to lead a discussion on what difference we have made.

\paragraph*{Extensible Parsing} Extensible parsing is achieved in many different areas, of which parser generators are a mainstream area specially designed for modular syntax and parsing. Many parser generators [OMeta, ANTLR, Rats!, noa, Ohm] \haoyuan{correct for Ohm?} support modular grammars, more specifically, they allow users to create new modules where new non-terminals and production rules can be introduced, some can even override existing rules in the old grammar modules. For instance, \textit{Rats!} [Rats!]
constructs its own module system for the collection of grammars, while NOA [NOA] uses Java annotation processing to gather information together. Those parser combinators focus on the \textit{syntactic extensibility} of grammars, and rely on a whole compilation to generate a global parser, even with a slight modification. Some of them may statically check the correctness and unambiguity of grammars, but separate compilation and modular type-checking remain unsolved, together with the issue of performance.

Besides, macro systems like C preprocessor, C++ templates [..] and Racket [..], and other meta-programming techniques [..] are a similar area aiming at syntactic extensibility as well, which sacrifices type safety. SugarJ [] is another well-known tool that conveniently introduces syntactic sugars in Java programming by library imports. Composition of syntactic sugars is easy for users, whereas it requires many rounds of parsing and adaption, which highly affects efficiency of compilation, moreover, the implementation was based on SDF [] and Stratego [], which focused little on separate compilation. Extensible compilers like JastAdd [] and Polyglot [], however, are somehow more ambitious, but they involve extensible parsing mostly by parser generators as well, and they focus more on the extensions to a host language. Our library is designed for modular language parsers in a type-safe way, with flexible language composition. Although overriding existing production rules is tricky with our approach, we could perhaps make use of embedded domain-specific languages on top of parser combinators and transformations for overriding, but it is anyway an orthogonal issue. \haoyuan{???} \haoyuan{BTW what about Racket?} \haoyuan{what about metafront? it is a macro system but does it have type safety?} \haoyuan{Extensible syntax with lexical scoping?} \haoyuan{"Extensible syntax" proposes a system for extensible syntax, where users write EDSLs in their language with concrete syntax. Users can write rules for type-based disambiguation. But separate compilation is again not mentioned. Shall we mention that thesis?} \haoyuan{attribute grammars?}

On the other hand, extensible parsing algorithms are another area that indeed relates to separate compilation. Specifically, [MB] introduces \textit{parse table composition}, in which paper grammars are compiled to the generation of parse tables, which are DFAs or NFAs, later they can be composed by an algorithm, so as to provide separate compilation for parsing. Nevertheless, the generation of parse tables is rather expensive, and furthermore, our approach supports separate compilation as well as modular type-checking, and the idea is not restricted to Scala but applicable to many functional languages, on whose type system the safety of modular parsing can rely. The extensibility of parsing in our approach is further available at language composition and lexical level.
\haoyuan{I mentioned one shortcoming of our approach here, which is we cannot override existing production rules; another one is that we do not have explicit correspondence/relationship between abstract syntax and the parser.}

\paragraph*{Parser Combinators} Since [Recursive Programming Techniques 1975] firstly introduced parser generators, and after Wadler discussed more details on backtracking in [1985], parser combinators have been more and more popularly developed and used in the research area of parsing. Among them many parsing libraries produces recursive descent parsers by introducing functional, or more specifically, monadic parser combinators with a foundation on [monadic parser combinators]. Related work includes Parsec [], which is frequently used in Haskell for context-sensitive grammars with infinite lookahead. Nevertheless, left-recursion is known as a big issue in recursive descent parsers, in which case programmers using libraries like Parsec have to manually eliminate left-recursion in the grammar, which is cumbersome, but also it distorts the shape of its old grammar, restricting the modularity of parsing to extreme extent.
There are certainly some approaches to afford the limitation of parser combinators, for example, we have conducted our previous experiment in Haskell, by using open recursion, MRM [] and Parsec. To support direct left-recursive grammars, we made use of the state monad in Parsec to ensure that a left-recursive production rule will not be applied successively in parsing. It turned out to be successful, yet introduced complexity in programming, hence we believe that the parsing library is obligated for left-recursion.

Some more recent papers [Packrat 2006, parsing with derivatives 2011, parser combinator for ambiguous left-recursive grammars] proposed a series of novel parsing techniques, moreover, they did put an eye on the issue of left-recursion. As we explained before, we have selected Packrat Parsing for our prototype implementation, as the combinators are convenient to use in Scala, which is a platform where functional laziness and Object Algebras can perfectly coexist. Also, its support for direct left-recursion is already helpful for designing real-world parsers practically, though [packrat parsing can support left recursion] further demonstrated that general left-recursion is supported from the theoretical point of view. We have also mentioned that the components used in our library can have alternatives, for instance, a different set of parser combinators may support ambiguous and left-recursive grammars, or be applicable to a different subset of context-free grammars or more, or even lead us to another level of performance.

Another important issue in previous papers, to the best of our knowledge, is that none of them deal with modular datatypes or abstract syntax trees. \haoyuan{Well, I need to go to check JS and Patrick Bahr's paper.} The modularity of data structures is yet necessary for modular parsing that supports separate compilation and modular type-checking. In our library, Object Algebras have been adopted for extensible ASTs, together with the behaviors on them.

\paragraph*{Modular Datatypes} Text.
