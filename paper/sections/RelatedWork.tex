\section{Related Work}\label{sec:relatedwork}

%- extensible parsing, language workbenches: rats, noa (this one already uses OA), modular semantic actions, (syntactic modularity, no separate compilation, modular type-checking)
%(read more papers, see if they talk about this issue, some potential solutions)
%(attribute grammars?)
%
%- parser combinators for type-checking, previous work has not shown how to support modularity (ASTs); left-recursion and back-tracking in related techniques
%
%- modularity: object algebras, dtc and mrm (problem with parsing, is there any related work? (PB: a paper on unfolds: build the AST))
%
%(parsing in Javascript: using delegation, does it support modular AST)
%
%noa, shy: shy: only override some interesting cases (transformation is tedious)
%bruijn indices: parsing + transformation

Our work integrates several components including extensible parsing,
parser combinators and extensibility techniques. There has been a
great amount of related papers on those hot topics. Some
inspired us of this paper and encourage us for more exploration. This
section will try to lead a discussion on what difference we have made.

\paragraph{Syntactically Extensible Parsing} Extensible parsing is achieved in many
different areas, among them parser generators are a mainstream area
specially designed for modular syntax and parsing. Many parser
generators~\cite{antlr1995,Grimm2006,Gouseti2014,Warth2016} support
modular grammars. They allow users to create new
modules where new non-terminals and production rules can be
introduced, some can even override existing rules in the old grammar
modules. For instance, \textit{Rats!}~\cite{Grimm2006} constructs its
own module system for the collection of grammars, while
NOA~\cite{Gouseti2014} uses Java annotation processing to gather
information together. Those parser combinators focus on the
\textit{syntactic extensibility} of grammars, and rely on a whole
compilation to generate a global parser, even with a slight
modification. Some of them may statically check the correctness and
unambiguity of grammars, but separate compilation and modular
type-checking remain unsolved, together with their affects on the performance.
\bruno{what performance issue?}\haoyuan{Revised}

Macro systems like C preprocessor, C++ templates and
Racket~\cite{Tobin-Hochstadt2011}, and other meta-programming
techniques are a similar area aiming at syntactic extensibility.
SugarJ~\cite{Erdweg2011} conveniently introduces syntactic sugar
for Java using library imports. Composition of syntactic
sugar is easy for users, but it requires many rounds of parsing
and adaption. This significantly affects efficiency of compilation,
moreover, the implementation was based on SDF~\cite{Heering1989} and
Stratego~\cite{Visser2001}, which did not focus on separate
compilation.\bruno{focused little or did not have it at all?}.\haoyuan{The authors have
mentioned separate compilation as a possible yet unimportant feature; they decided to discard it.}
Racket adopts macro system for library-based language extensibility. It is powerful with attributed
ASTs for contextual information, and extensions can be integrated in a modular way. Whereas
such modularity is not flexible enough for language unification, as the syntax is only built from extensions.
Moreover, existing macros cannot be further changed after definition. \haoyuan{correct?}
On the other hand, extensible compilers like JastAdd~\cite{Ekman2007} and
Polyglot~\cite{Nystrom2003} also support extensible parsing, but this
is mostly done using parser generators. Moreover
they focus on the extensions to a host language. Our library
is designed for modular language parsers in a type-safe way, with
flexible language composition.
%\bruno{Have you read
%Languages as Libraries? That is probably an important reference, which
%I think we should cite.}
%\haoyuan{what about
%  metafront? it is a macro system but does it have type safety?}
%\haoyuan{Extensible syntax with lexical scoping?} \haoyuan{"Extensible
%  syntax" proposes a system for extensible syntax, where users write
%  EDSLs in their language with concrete syntax. Users can write rules
%  for type-based disambiguation. But separate compilation is again not
%  mentioned. Shall we mention that thesis?} \haoyuan{attribute
%  grammars?}

\paragraph{Extensible Parsing Algorithms}
Extensible parsing algorithms are another area that indeed relates to
separate compilation. Specifically, \cite{Bravenboer2009} introduces
\textit{parse table composition}, where grammars are compiled
to the generation of parse tables. Those parse tables are expressed as DFAs or NFAs, later they
can be composed by an algorithm, so as to provide separate compilation
for parsing. Nevertheless, the generation of parse tables is rather
expensive in terms of performance.
Furthermore, our approach supports \emph{both} separate compilation
as well as \emph{modular type-checking}. The extensibility of
parsing is further available at language composition
and lexical level.
%\haoyuan{we do not have explicit
%  correspondence/relationship between abstract syntax and the parser.}

\paragraph{Parser Combinators} Since~\cite{burge1975} firstly
introduced parser generators, and after Wadler discussed more details
on backtracking in~\cite{Wadler1985}, parser combinators have been
more and more popularly developed and used in the research area of
parsing. Among them many parsing libraries produces recursive descent
parsers by introducing functional, or more specifically, monadic
parser combinators with a foundation on~\cite{nott237}. Related work
includes Parsec~\cite{Leijen2001}, which is frequently used in Haskell
for context-sensitive grammars with infinite lookahead. Nevertheless,
Parsec users suffer from manual left-recursion elimination, back-tracking and
longest match composition issues, as we discussed before.

Some more recent papers~\cite{Ford2002,Might2011,Frost2008} proposed a series of
novel parsing techniques that addressed the issue
of left-recursion. We have selected Packrat
Parsing for our prototype implementation, as the combinators are
convenient to use in Scala, which is a platform where functional
laziness and Object Algebras can perfectly coexist. Also, its support
for direct left-recursion is already helpful for designing real-world
parsers practically, though~\cite{warth2008} further demonstrated that
general left-recursion is supported theoretically.
Yet we have mentioned that the components used in our approach
can have alternatives to achieve better modular parsing.




\paragraph{Extensibility} To the best of our knowledge, none of the existing techniques
address the problem of producing extensible ASTs. Certainly, there has been a lot of work on extensible
data types, in both functional and object-oriented programming.
For instance, the famous DTC paper~\cite{swierstra2008} represents
modular ASTs using co-products of every two functors, which forms a
binary tree structure. Later
Oliveira et al. presented MRM~\cite{Oliveira2015} in Haskell, which is
similar to DTC but adopts a list of functors in type-level lists
instead. Other related work
includes~\cite{Bahr2014}, where data types are flexible with
composition as well as decomposition.

In OO languages, however, things get much better with the mechanism of
inheritance. Furthermore, Object Algebras~\cite{Oliveira2012} were
proposed as a design pattern for the extensibility of both data
variants and operations in a modular type-safe way. The code is
concise and expressive using Object Algebras, building a foundation
for our new library with various dimensions of extensibility.
Moreover, in~\cite{Oliveira2012} the authors have discussed the
composition of algebras. In our parsing approach, a parser consumes an
algebra, which is delegated to return the results, during its process
of parsing. Having a set of algebras, it requires multiple parsing
with several times of invocation, which leads to redundant work.
Instead, algebras are supposed to be composed into one before the
invocation of the parser. Bahr et al. lead a similar discussion
in~\cite{Bahr2011}, where queries (or \textit{catamorphisms}) and
transformations (or \textit{homomorphisms}) are composable. They have
also mentioned the dual process of folds, namely
\textit{anamorphisms}. It is potentially related to our work, as
parsing is a representative kind of unfolds, whereas they only
discussed the composition of a cv-coalgebra and a term homomorphism,
which differs from modular parsing.

Another interesting observation, as we have mentioned
in~\ref{subsec:objectalgebras}, is that transformation algebras are
frequently used in parsing, as concrete syntax may not always coincide
with its abstract syntax, for instance, the bruijn indices in lambda
calculus, as well as other syntactic sugars. In the Shy
framework~\cite{Zhang2015}, the pattern of transformations is captured
into a template for Java annotation processing, and most of
boilerplate code is automatically generated, hence users only need to
override a few interesting cases, conveniently. It is potentially useful
in our case. By the way, the idea of overriding
existing cases was also proposed in MRM. Our approach allows existing parsing
code to be overridden during inheritance.