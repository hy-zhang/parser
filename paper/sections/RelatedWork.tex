\section{Related Work}\label{sec:relatedwork}

- extensible parsing, language workbenches: rats, noa (this one already uses OA), modular semantic actions, (syntactic modularity, no separate compilation, modular type-checking)
(read more papers, see if they talk about this issue, some potential solutions)
(attribute grammars?)

- parser combinators for type-checking, previous work has not shown how to support modularity (ASTs); left-recursion and back-tracking in related techniques

- modularity: object algebras, dtc and mrm (problem with parsing, is there any related work? (PB: a paper on unfolds: build the AST))

(parsing in Javascript: using delegation, does it support modular AST)

noa, shy: shy: only override some interesting cases (transformation is tedious)
bruijn indices: parsing + transformation

Our work integrates several components including extensible parsing, parser combinators and modular datatypes. There has been a great amount of related papers
on those hot topics, of which some inspired us of this paper and encourages us for more exploration. This section will try to lead a discussion on what difference we have made.

\paragraph*{Extensible Parsing} Extensible parsing is achieved in many different areas, of which parser generators are a mainstream area specially designed for modular syntax and parsing. Many parser generators [OMeta, ANTLR, Rats!, noa, Ohm] \haoyuan{correct for Ohm?} support modular grammars, more specifically, they allow users to create new modules where new non-terminals and production rules can be introduced, some can even override existing rules in the old grammar modules. For instance, \textit{Rats!} [Rats!]
constructs its own module system for the collection of grammars, while NOA [NOA] uses Java annotation processing to gather information together. Those parser combinators focus on the \textit{syntactic extensibility} of grammars, and rely on a whole compilation to generate a global parser, even with a slight modification. Some of them may statically check the correctness and unambiguity of grammars, but separate compilation and modular type-checking remain unsolved, together with the issue of performance.

Besides, macro systems like C preprocessor, C++ templates [..] and Racket [..], and other meta-programming techniques [..] are a similar area aiming at syntactic extensibility as well, which sacrifices type safety. SugarJ [] is another well-known tool that conveniently introduces syntactic sugars in Java programming by library imports. Composition of syntactic sugars is easy for users, whereas it requires many rounds of parsing and adaption, which highly affects efficiency of compilation, moreover, the implementation was based on SDF [] and Stratego [], which focused little on separate compilation. Extensible compilers like JastAdd [] and Polyglot [], however, are somehow more ambitious, but they involve extensible parsing mostly by parser generators as well, and they focus more on the extensions to a host language. Our library is designed for modular language parsers in a type-safe way, with flexible language composition. Although overriding existing production rules is tricky with our approach, we could perhaps make use of embedded domain-specific languages on top of parser combinators and transformations for overriding, but it is anyway an orthogonal issue. \haoyuan{???} \haoyuan{BTW what about Racket?} \haoyuan{what about metafront? it is a macro system but does it have type safety?} \haoyuan{Extensible syntax with lexical scoping?} \haoyuan{"Extensible syntax" proposes a system for extensible syntax, where users write EDSLs in their language with concrete syntax. Users can write rules for type-based disambiguation. But separate compilation is again not mentioned. Shall we mention that thesis?} \haoyuan{attribute grammars?}

On the other hand, extensible parsing algorithms are another area that indeed relates to separate compilation. Specifically, [MB] introduces \textit{parse table composition}, in which paper grammars are compiled to the generation of parse tables, which are DFAs or NFAs, later they can be composed by an algorithm, so as to provide separate compilation for parsing. Nevertheless, the generation of parse tables is rather expensive, and furthermore, our approach supports separate compilation as well as modular type-checking, and the idea is not restricted to Scala but applicable to many functional languages, on whose type system the safety of modular parsing can rely. The extensibility of parsing in our approach is further available at language composition and lexical level.
\haoyuan{I mentioned one shortcoming of our approach here, which is we cannot override existing production rules; another one is that we do not have explicit correspondence/relationship between abstract syntax and the parser.}

\paragraph*{Parser Combinators} Text.

\paragraph*{Modular Datatypes} Text.
